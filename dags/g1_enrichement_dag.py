from __future__ import annotations

import pendulum
from airflow.decorators import dag, task
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.operators.bash import BashOperator

@dag(
    dag_id="g1_enrichment_pipeline",
    schedule_interval="0 9 * * *",  # Executa Ã s 9h, 1 hora depois do scraping
    start_date=pendulum.datetime(2025, 9, 5, tz="America/Sao_Paulo"),
    catchup=False,
    tags=["enrichment", "llm", "g1", "silver", "ai"],
    doc_md="""
    ### Pipeline de Enriquecimento de NotÃ­cias do G1 com IA
    Esta DAG Ã© responsÃ¡vel por:
    1. Aguardar a conclusÃ£o bem-sucedida da DAG g1_scraping_pipeline (sensor).
    2. Criar a tabela silver de dados enriquecidos no PostgreSQL.
    3. Executar o script de enriquecimento com LLM para analisar manchetes.
    4. Processar manchetes ainda nÃ£o analisadas da camada Bronze.
    5. Classificar sentimento e categorizar usando OpenAI GPT.
    6. Salvar os dados enriquecidos na camada Silver.
    
    **ConfiguraÃ§Ã£o:**
    - SCHEDULE: 9h diÃ¡rio (1 hora apÃ³s o scraping Ã s 8h)
    - SAFETY CHECK: ExternalTaskSensor aguarda scraping finalizar
    - MANUAL: Pode ser executada manualmente
    
    **DependÃªncias:**
    - AGUARDA: g1_scraping_pipeline executar com sucesso
    - Necessita das variÃ¡veis de ambiente: OPENAI_API_KEY, POSTGRES_*
    
    **Tabelas envolvidas:**
    - Input: raw_headlines (Bronze layer)
    - Output: silver_enriched_headlines (Silver layer)
    """
)
def g1_enrichment_pipeline():
    """
    DAG que orquestra o processo de enriquecimento de manchetes com IA.
    """
    
    # Tarefa 1: Criar tabela silver se nÃ£o existir - ESTRUTURA CORRIGIDA
    create_silver_table = PostgresOperator(
        task_id="create_silver_enriched_table",
        postgres_conn_id="postgres_default",
        sql="""
            CREATE TABLE IF NOT EXISTS silver_enriched_headlines (
                id SERIAL PRIMARY KEY,
                raw_link TEXT NOT NULL,
                title TEXT NOT NULL,
                link TEXT,
                source TEXT,
                scraped_at TIMESTAMP,
                sentiment VARCHAR(20),
                category VARCHAR(50),
                confidence_score FLOAT,
                processing_time_seconds FLOAT,
                processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                model_used VARCHAR(50) DEFAULT 'gpt-3.5-turbo-1106'
            );
            
            -- Criar Ã­ndice para evitar duplicatas usando raw_link
            CREATE UNIQUE INDEX IF NOT EXISTS idx_silver_raw_link 
            ON silver_enriched_headlines(raw_link);
            
            -- Criar Ã­ndices para consultas
            CREATE INDEX IF NOT EXISTS idx_silver_sentiment 
            ON silver_enriched_headlines(sentiment);
            
            CREATE INDEX IF NOT EXISTS idx_silver_category 
            ON silver_enriched_headlines(category);
            
            CREATE INDEX IF NOT EXISTS idx_silver_processed_at 
            ON silver_enriched_headlines(processed_at);
        """
    )

    # Tarefa 2: Verificar se hÃ¡ dados novos para processar - QUERY CORRIGIDA
    @task
    def check_pending_headlines():
        """
        Verifica se existem manchetes pendentes de processamento.
        """
        from airflow.providers.postgres.hooks.postgres import PostgresHook
        import logging
        
        logger = logging.getLogger(__name__)
        hook = PostgresHook(postgres_conn_id='postgres_default')
        
        # Query corrigida para usar link como chave
        query = """
        SELECT COUNT(*) as pending_count
        FROM raw_headlines r
        LEFT JOIN silver_enriched_headlines s ON r.link = s.raw_link
        WHERE s.raw_link IS NULL
        """
        
        result = hook.get_first(query)
        pending_count = result[0] if result else 0
        
        logger.info(f"Encontradas {pending_count} manchetes pendentes de enriquecimento")
        
        if pending_count == 0:
            logger.info("Nenhuma manchete nova para processar. Pulando enriquecimento.")
            return False
        
        return True

    # Tarefa 3: Executar enriquecimento com LLM
    run_llm_enricher = BashOperator(
        task_id="run_llm_enricher",
        bash_command="cd /opt/airflow && python scripts/llm_enricher.py",
        env={
            'PYTHONPATH': '/opt/airflow',
        }
    )

    # Tarefa 4: Validar qualidade dos dados enriquecidos
    @task
    def validate_enriched_data():
        """
        Valida a qualidade dos dados enriquecidos recÃ©m-processados.
        """
        from airflow.providers.postgres.hooks.postgres import PostgresHook
        import logging
        
        logger = logging.getLogger(__name__)
        hook = PostgresHook(postgres_conn_id='postgres_default')
        
        # Verificar dados processados hoje
        validation_queries = [
            ("Total processado hoje", """
                SELECT COUNT(*) FROM silver_enriched_headlines 
                WHERE DATE(processed_at) = CURRENT_DATE
            """),
            ("Registros com erro hoje", """
                SELECT COUNT(*) FROM silver_enriched_headlines 
                WHERE DATE(processed_at) = CURRENT_DATE 
                AND (sentiment = 'Erro' OR category = 'Erro')
            """),
            ("Taxa de confianÃ§a mÃ©dia hoje", """
                SELECT ROUND(AVG(confidence_score)::numeric, 3) FROM silver_enriched_headlines 
                WHERE DATE(processed_at) = CURRENT_DATE 
                AND sentiment != 'Erro'
            """),
            ("DistribuiÃ§Ã£o de sentimentos hoje", """
                SELECT sentiment, COUNT(*) FROM silver_enriched_headlines 
                WHERE DATE(processed_at) = CURRENT_DATE 
                GROUP BY sentiment
            """)
        ]
        
        validation_results = {}
        
        for description, query in validation_queries:
            try:
                if "DistribuiÃ§Ã£o" in description:
                    results = hook.get_records(query)
                    validation_results[description] = results
                    logger.info(f"{description}: {results}")
                else:
                    result = hook.get_first(query)
                    value = result[0] if result and result[0] is not None else 0
                    validation_results[description] = value
                    logger.info(f"{description}: {value}")
            except Exception as e:
                logger.error(f"Erro ao executar validaÃ§Ã£o '{description}': {e}")
                validation_results[description] = "Erro"
        
        # VerificaÃ§Ãµes de qualidade
        total_today = validation_results.get("Total processado hoje", 0)
        errors_today = validation_results.get("Registros com erro hoje", 0)
        avg_confidence = validation_results.get("Taxa de confianÃ§a mÃ©dia hoje", 0)
        
        if total_today == 0:
            logger.warning("Nenhum registro foi processado hoje!")
        
        if total_today > 0:
            error_rate = (errors_today / total_today) * 100
            logger.info(f"Taxa de erro hoje: {error_rate:.1f}%")
            
            if error_rate > 10:  # Se mais de 10% tiveram erro
                logger.warning(f"Taxa de erro alta: {error_rate:.1f}%")
            
            if avg_confidence and avg_confidence < 0.7:
                logger.warning(f"ConfianÃ§a mÃ©dia baixa: {avg_confidence}")
        
        return validation_results

    # Tarefa 5: Gerar relatÃ³rio de processamento
    @task
    def generate_processing_report():
        """
        Gera relatÃ³rio detalhado do processamento.
        """
        from airflow.providers.postgres.hooks.postgres import PostgresHook
        import logging
        import json
        
        logger = logging.getLogger(__name__)
        hook = PostgresHook(postgres_conn_id='postgres_default')
        
        # EstatÃ­sticas gerais
        stats_queries = {
            "total_raw": "SELECT COUNT(*) FROM raw_headlines",
            "total_processed": "SELECT COUNT(*) FROM silver_enriched_headlines",
            "processed_today": """
                SELECT COUNT(*) FROM silver_enriched_headlines 
                WHERE DATE(processed_at) = CURRENT_DATE
            """,
            "avg_processing_time": """
                SELECT ROUND(AVG(processing_time_seconds)::numeric, 3) 
                FROM silver_enriched_headlines 
                WHERE DATE(processed_at) = CURRENT_DATE
            """
        }
        
        report = {}
        for key, query in stats_queries.items():
            try:
                result = hook.get_first(query)
                report[key] = result[0] if result and result[0] is not None else 0
            except Exception as e:
                logger.error(f"Erro ao executar query '{key}': {e}")
                report[key] = 0
        
        # Calcular pendentes
        report["pending"] = report["total_raw"] - report["total_processed"]
        
        # RelatÃ³rio de categorias hoje
        try:
            category_results = hook.get_records("""
                SELECT category, COUNT(*) FROM silver_enriched_headlines 
                WHERE DATE(processed_at) = CURRENT_DATE 
                AND category != 'Erro'
                GROUP BY category 
                ORDER BY COUNT(*) DESC
            """)
            report["categories_today"] = category_results
        except:
            report["categories_today"] = []
        
        # Log do relatÃ³rio
        logger.info("ðŸ“Š RELATÃ“RIO DE ENRIQUECIMENTO:")
        logger.info(f"   Total manchetes: {report['total_raw']}")
        logger.info(f"   Total processadas: {report['total_processed']}")
        logger.info(f"   Processadas hoje: {report['processed_today']}")
        logger.info(f"   Pendentes: {report['pending']}")
        logger.info(f"   Tempo mÃ©dio por manchete: {report['avg_processing_time']}s")
        
        if report['categories_today']:
            logger.info("   Categorias processadas hoje:")
            for category, count in report['categories_today'][:5]:  # Top 5
                logger.info(f"     â€¢ {category}: {count}")
        
        return report

    # Definir dependÃªncias
    check_task = check_pending_headlines()
    validate_task = validate_enriched_data()
    report_task = generate_processing_report()
    
    # Fluxo simplificado sem sensor externo
    create_silver_table >> check_task
    check_task >> run_llm_enricher >> validate_task >> report_task

# Instanciar a DAG
g1_enrichment_pipeline()