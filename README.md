# AirDataPipeline

<!-- Badges das principais tecnologias -->
<p align="center">
  <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python"/>
  <img src="https://img.shields.io/badge/Apache%20Airflow-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white" alt="Apache Airflow"/>
  <img src="https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white" alt="Docker"/>
  <img src="https://img.shields.io/badge/OpenAI-412991?style=for-the-badge&logo=openai&logoColor=white" alt="OpenAI"/>
  <img src="https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white" alt="PostgreSQL"/>
  <img src="https://img.shields.io/badge/DBT-FF694B?style=for-the-badge&logo=dbt&logoColor=white" alt="DBT"/>
  <img src="https://img.shields.io/badge/ETL-FF6B35?style=for-the-badge&logo=databricks&logoColor=white" alt="ETL Pipeline"/>
</p>
 

Um pipeline completo de Engenharia de Dados para coleta, processamento e visualizaÃ§Ã£o de notÃ­cias do portal G1, implementado com as principais ferramentas do mercado.

## ðŸŽ¯ VisÃ£o Geral

Este projeto demonstra a implementaÃ§Ã£o de um pipeline de dados moderno (Data Lakehouse) seguindo a arquitetura **Bronze â†’ Silver â†’ Gold**, com orquestraÃ§Ã£o automatizada e visualizaÃ§Ã£o interativa.

### Arquitetura do Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Scraping  â”‚â”€â”€â”€â–¶â”‚   Airflow    â”‚â”€â”€â”€â–¶â”‚ PostgreSQL  â”‚â”€â”€â”€â–¶â”‚  LLM AI      â”‚â”€â”€â”€â–¶â”‚     DBT      â”‚â”€â”€â”€â–¶â”‚  Streamlit   â”‚
â”‚     (G1)        â”‚    â”‚ (OrquestraÃ§Ã£o)â”‚   â”‚  (Bronze)   â”‚    â”‚(Enriquecimento)â”‚   â”‚(Silver/Gold) â”‚    â”‚ (Dashboard)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                      â”‚
                                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                              â”‚ â€¢ Sentimento â”‚
                                                              â”‚ â€¢ Categoria  â”‚
                                                              â”‚ â€¢ Tags       â”‚
                                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ—ï¸ Componentes da Arquitetura

### ðŸ“° **Fonte de Dados (Web Scraping)**
**Nosso fornecedor de matÃ©ria-prima**

- **Alvo**: Portal G1 (https://g1.globo.com/)
- **Tecnologia**: Python + Playwright (para conteÃºdo dinÃ¢mico) / BeautifulSoup + Requests (para conteÃºdo estÃ¡tico)
- **Dados coletados**: Manchetes, links, timestamp de coleta
- **FrequÃªncia**: Agendada via Airflow
- **LocalizaÃ§Ã£o**: `scripts/scraper.py`

### ðŸ§  **Orquestrador (Apache Airflow)**
**O "cÃ©rebro" da operaÃ§Ã£o**

- **FunÃ§Ã£o**: AutomatizaÃ§Ã£o e agendamento das tarefas
- **Recursos**: 
  - ExecuÃ§Ã£o agendada (ex: todos os dias Ã s 8h)
  - Monitoramento de falhas e retries
  - Logs detalhados de execuÃ§Ã£o
  - Interface web para acompanhamento
- **LocalizaÃ§Ã£o**: `dags/`

### ðŸ›ï¸ **Banco de Dados (PostgreSQL)**
**Nosso "armazÃ©m" - Bronze Layer**

- **FunÃ§Ã£o**: Armazenamento dos dados brutos
- **CaracterÃ­sticas**:
  - Dados exatamente como coletados
  - Sem transformaÃ§Ãµes
  - HistÃ³rico completo preservado
- **ConfiguraÃ§Ã£o**: Docker container

### ðŸ¤– **Enriquecimento com IA (LLM)**
**Nossa "inteligÃªncia analÃ­tica"**

- **FunÃ§Ã£o**: AnÃ¡lise e classificaÃ§Ã£o automÃ¡tica das manchetes
- **Tecnologias**: OpenAI GPT / Anthropic Claude / Modelos locais (Ollama)
- **Processamento**:
  - **AnÃ¡lise de Sentimento**: Classifica cada manchete como:
    - ðŸŸ¢ **Positiva**: NotÃ­cias otimistas, conquistas, boas notÃ­cias
    - ðŸ”´ **Negativa**: TragÃ©dias, conflitos, problemas sociais
    - âšª **Neutra**: InformaÃ§Ãµes factuais, declaraÃ§Ãµes, eventos neutros
  - **CategorizaÃ§Ã£o**: Classifica por tÃ³picos:
    - ðŸ›ï¸ **PolÃ­tica**: Governo, eleiÃ§Ãµes, polÃ­ticas pÃºblicas
    - âš½ **Esportes**: Futebol, olimpÃ­adas, competiÃ§Ãµes
    - ðŸ’» **Tecnologia**: InovaÃ§Ãµes, ciÃªncia, digital
    - ðŸ’° **Economia**: Mercado, inflaÃ§Ã£o, negÃ³cios
    - ðŸŽ­ **Cultura**: Arte, entretenimento, celebridades
    - ðŸ¥ **SaÃºde**: Medicina, epidemias, bem-estar
    - ðŸŒ **Internacional**: NotÃ­cias globais, diplomacia
    - âš–ï¸ **JustiÃ§a**: Crimes, tribunais, legislaÃ§Ã£o
- **Posicionamento**: Entre Bronze Layer (dados brutos) e Silver Layer (dados limpos)
- **BenefÃ­cios**:
  - AutomaÃ§Ã£o de classificaÃ§Ã£o manual
  - AnÃ¡lises de tendÃªncias de sentimento
  - SegmentaÃ§Ã£o inteligente de conteÃºdo
  - Insights sobre padrÃµes noticiosos

### âš™ï¸ **Ferramenta de TransformaÃ§Ã£o (DBT)**
**Nossa "linha de beneficiamento"**

- **Bronze â†’ Silver**: Limpeza, padronizaÃ§Ã£o + dados enriquecidos pela IA
- **Silver â†’ Gold**: Modelagem analÃ­tica e agregaÃ§Ãµes por sentimento/categoria
- **Tecnologia**: SQL puro
- **BenefÃ­cios**:
  - Versionamento de transformaÃ§Ãµes
  - Testes de qualidade de dados
  - DocumentaÃ§Ã£o automÃ¡tica
  - MÃ©tricas analÃ­ticas avanÃ§adas com IA
- **LocalizaÃ§Ã£o**: `dbt_project/`

#### Modelos DBT Implementados

O DBT organiza as transformaÃ§Ãµes em trÃªs camadas principais:

1. **Bronze** (Dados brutos):
   - `raw_headlines`: Carrega os dados brutos das manchetes do G1
   - `raw_enriched_headlines`: Dados brutos com enriquecimento de IA

2. **Silver** (Dados limpos):
   - `clean_headlines`: Limpeza e normalizaÃ§Ã£o das manchetes
   - `enriched_headlines`: Dados limpos + classificaÃ§Ãµes de IA
   - `news_categories`: Taxonomia padronizada das categorias de notÃ­cias
   - `sentiment_metrics`: MÃ©tricas de sentimento por manchete

3. **Gold** (Camada analÃ­tica):
   - `daily_sentiment_analysis`: AgregaÃ§Ã£o diÃ¡ria de sentimentos por categoria
   - `category_distribution`: DistribuiÃ§Ã£o de notÃ­cias por categoria ao longo do tempo
   - `trending_topics`: IdentificaÃ§Ã£o dos tÃ³picos em alta por perÃ­odo
   - `sentiment_trends`: AnÃ¡lise de tendÃªncias de sentimento por perÃ­odo

#### Como Executar o DBT

Para configurar e executar as transformaÃ§Ãµes DBT:

```bash
# Navegue atÃ© o diretÃ³rio do projeto DBT
cd dbt_project

# Verifique se as conexÃµes estÃ£o configuradas corretamente
dbt debug

# Execute todos os modelos
dbt run

# Execute modelos especÃ­ficos
dbt run --models +silver.enriched_headlines+

# Execute modelos por tag
dbt run --models tag:daily

# Gere documentaÃ§Ã£o
dbt docs generate
dbt docs serve
```

#### Arquivo de Perfil DBT

O DBT requer um arquivo de configuraÃ§Ã£o de perfil em `~/.dbt/profiles.yml`:

```yaml
dbt_project:
  target: dev
  outputs:
    dev:
      type: postgres
      host: localhost
      user: airflow
      password: airflow
      port: 5432
      dbname: airflow
      schema: dbt_gold
      threads: 4
```

#### Testes e Qualidade de Dados

O projeto inclui testes para garantir a qualidade dos dados:

```bash
# Execute todos os testes
dbt test

# Execute testes especÃ­ficos
dbt test --models silver.enriched_headlines
```

### ðŸ“Š **Camada de VisualizaÃ§Ã£o (Streamlit)**
**Nossa "vitrine"**

- **FunÃ§Ã£o**: Dashboard interativo com insights de IA
- **Recursos**:
  - VisualizaÃ§Ã£o dos dados da camada Gold
  - **AnÃ¡lises de Sentimento**: DistribuiÃ§Ã£o temporal de sentimentos
  - **Dashboard de Categorias**: Volume por tÃ³pico e tendÃªncias
  - **Palavra Cloud**: Termos mais frequentes por categoria
  - **MÃ©tricas de IA**: PrecisÃ£o da classificaÃ§Ã£o e confianÃ§a
  - **Alertas**: DetecÃ§Ã£o de picos de sentimento negativo
- **LocalizaÃ§Ã£o**: `streamlit_app/`

### ðŸ³ **Ambiente (Docker & Docker Compose)**
**Nossa "fÃ¡brica"**

- **Docker**: ContainerizaÃ§Ã£o de cada componente
- **Docker Compose**: OrquestraÃ§Ã£o de todos os serviÃ§os
- **BenefÃ­cios**:
  - Ambiente reproduzÃ­vel
  - Isolamento de dependÃªncias
  - Deploy simplificado

## ðŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### PrÃ©-requisitos

- Python 3.10+
- Docker e Docker Compose
- Git

### 1. Clone o RepositÃ³rio

```bash
git clone https://github.com/edu-data-dev/AirDataPipeline.git
cd AirDataPipeline
```

### 2. ConfiguraÃ§Ã£o do Ambiente Python

```bash
# Criar ambiente virtual
python -m venv .venv

# Ativar ambiente virtual (Linux/Mac)
source .venv/bin/activate

# Ativar ambiente virtual (Windows)
.venv\Scripts\activate

# Instalar dependÃªncias
pip install -r requirements.txt

# Instalar navegador para Playwright
python -m playwright install chromium
```

### 3. ConfiguraÃ§Ã£o do Ambiente

```bash
# Copiar arquivo de configuraÃ§Ã£o
cp .env.example .env

# Editar variÃ¡veis de ambiente conforme necessÃ¡rio
nano .env
```

## ðŸ“‹ Uso do Sistema

### Web Scraping Manual

O scraper suporta duas engines:

#### Engine Requests (conteÃºdo estÃ¡tico)
```bash
python scripts/scraper.py --engine requests
```

#### Engine Playwright (conteÃºdo dinÃ¢mico - Recomendado)
```bash
python scripts/scraper.py --engine playwright
```

**SaÃ­da**: Arquivo CSV com timestamp (`g1_headlines_playwright_YYYYMMDD_HHMMSS.csv`)

### Exemplo de Dados Coletados

```csv
title,link,source,scraped_at
"Corpo de Luis Fernando Verissimo Ã© velado na Assembleia do RS",https://g1.globo.com/rs/rio-grande-do-sul/noticia/2025/08/30/corpo-do-escritor-luis-fernando-verissimo-e-velado-na-assembleia-legislativa-em-porto-alegre.ghtml,G1,2025-08-30T18:00:06.860299
"Ã‰ #FAKE que Trump morreu; presidente Ã© visto a caminho de campo de golfe",https://g1.globo.com/fato-ou-fake/noticia/2025/08/30/e-fake-que-donald-trump-morreu-presidente-e-visto-a-caminho-de-golfe.ghtml,G1,2025-08-30T18:00:07.026369
```

### Exemplo de Dados Enriquecidos com IA (Futuro)

```csv
title,link,source,scraped_at,sentiment,sentiment_score,category,category_score
"Corpo de Luis Fernando Verissimo Ã© velado na Assembleia do RS",https://g1.globo.com/...,G1,2025-08-30T18:00:06.860299,Negativa,0.85,Cultura,0.92
"Ã‰ #FAKE que Trump morreu; presidente Ã© visto a caminho de campo de golfe",https://g1.globo.com/...,G1,2025-08-30T18:00:07.026369,Neutra,0.78,PolÃ­tica,0.89
```

### Subir o Ambiente Completo

```bash
# Iniciar todos os serviÃ§os
docker-compose up -d

# Verificar status
docker-compose ps

# Acessar logs
docker-compose logs -f [serviÃ§o]
```

### Interfaces de Acesso

- **Airflow**: http://localhost:8080
- **Streamlit Dashboard**: http://localhost:8501
- **PostgreSQL**: localhost:5432

### Usando o DBT para TransformaÃ§Ãµes de Dados

O DBT (Data Build Tool) Ã© usado para transformar os dados coletados pelo scraper e enriquecidos pela IA em modelos analÃ­ticos prontos para visualizaÃ§Ã£o.

#### ConfiguraÃ§Ã£o Inicial do DBT

```bash
# Certifique-se de que o arquivo de perfil estÃ¡ configurado
mkdir -p ~/.dbt
cat > ~/.dbt/profiles.yml << 'EOF'
dbt_project:
  target: dev
  outputs:
    dev:
      type: postgres
      host: localhost
      user: airflow
      password: airflow
      port: 5432
      dbname: airflow
      schema: dbt_gold
      threads: 4
EOF
```

#### Fluxo de Trabalho com DBT

1. **Dados Brutos (Bronze)**:
   ```bash
   cd dbt_project
   dbt run --models bronze
   ```
   Este comando carrega os dados coletados do scraper para a camada bronze.

2. **Dados Limpos (Silver)**:
   ```bash
   dbt run --models silver
   ```
   Este comando transforma os dados brutos em dados limpos e estruturados, incorporando o enriquecimento da IA.

3. **Dados AnalÃ­ticos (Gold)**:
   ```bash
   dbt run --models gold
   ```
   Este comando cria as mÃ©tricas e agregaÃ§Ãµes para anÃ¡lise e visualizaÃ§Ã£o.

4. **ExecuÃ§Ã£o Completa**:
   ```bash
   dbt run
   ```
   Este comando executa todas as transformaÃ§Ãµes em sequÃªncia.

5. **ValidaÃ§Ã£o de Qualidade**:
   ```bash
   dbt test
   ```
   Este comando executa testes para garantir a qualidade dos dados em todas as camadas.

## ðŸ“ Estrutura do Projeto

```
projeto_eng_dados-01/
â”œâ”€â”€ .env.example              # Exemplo de variÃ¡veis de ambiente
â”œâ”€â”€ .gitignore               # Arquivos ignorados pelo Git
â”œâ”€â”€ requirements.txt         # DependÃªncias Python
â”œâ”€â”€ docker-compose.yml       # ConfiguraÃ§Ã£o dos containers
â”œâ”€â”€ README.md               # Este arquivo
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ scraper.py          # Web scraper do G1
â”‚   â””â”€â”€ llm_enricher.py     # Enriquecimento com IA (sentimento/categoria)
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ scraping_dag.py     # Pipeline de coleta
â”‚   â””â”€â”€ enrichment_dag.py   # Pipeline de enriquecimento IA
â”œâ”€â”€ dbt_project/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ bronze/         # Dados brutos
â”‚   â”‚   â”œâ”€â”€ silver/         # Dados limpos + IA
â”‚   â”‚   â””â”€â”€ gold/           # MÃ©tricas analÃ­ticas
â”‚   â”œâ”€â”€ tests/              # Testes de qualidade
â”‚   â””â”€â”€ dbt_project.yml     # ConfiguraÃ§Ã£o DBT
â””â”€â”€ streamlit_app/
    â”œâ”€â”€ pages/
    â”‚   â”œâ”€â”€ sentiment_analysis.py  # Dashboard de sentimentos
    â”‚   â””â”€â”€ category_trends.py     # AnÃ¡lise por categorias
    â””â”€â”€ main.py             # Dashboard principal
```

## ðŸ”§ Funcionalidades Implementadas

### âœ… Web Scraping
- [x] Coleta de manchetes do G1
- [x] Suporte a conteÃºdo dinÃ¢mico (Playwright)
- [x] Fallback para conteÃºdo estÃ¡tico (Requests + BeautifulSoup)
- [x] DeduplicaÃ§Ã£o automÃ¡tica
- [x] Timestamps de coleta
- [x] Logs detalhados
- [x] Interface CLI com argumentos

### âœ… Infraestrutura
- [x] ContainerizaÃ§Ã£o com Docker
- [x] Gerenciamento de dependÃªncias
- [x] Controle de versÃ£o configurado
- [x] Ambiente virtual Python
- [x] ConfiguraÃ§Ã£o via variÃ¡veis de ambiente

### ðŸš§ Em Desenvolvimento
- [ ] MÃ³dulo de enriquecimento com IA
- [ ] DAGs do Airflow (coleta + IA)
- [x] Modelagem DBT com dados de IA
  - [x] Camada Bronze: Carregamento de dados brutos
  - [x] Camada Silver: Limpeza e enriquecimento
  - [x] Camada Gold: AgregaÃ§Ãµes e mÃ©tricas analÃ­ticas
  - [x] Testes de qualidade dos dados
  - [ ] DocumentaÃ§Ã£o completa dos modelos
- [ ] Dashboard Streamlit com anÃ¡lises de sentimento
- [ ] Testes automatizados
- [ ] CI/CD Pipeline

### ðŸ”® Recursos de IA Planejados
- [ ] **AnÃ¡lise de Sentimento**: ClassificaÃ§Ã£o automÃ¡tica Positiva/Negativa/Neutra
- [ ] **CategorizaÃ§Ã£o**: PolÃ­tica, Esportes, Tecnologia, Economia, Cultura, SaÃºde, Internacional, JustiÃ§a
- [ ] **MÃ©tricas de ConfianÃ§a**: Score de certeza da classificaÃ§Ã£o
- [ ] **DetecÃ§Ã£o de Trending Topics**: IdentificaÃ§Ã£o de assuntos em alta
- [ ] **Alertas Inteligentes**: NotificaÃ§Ãµes sobre picos de sentimento negativo
- [ ] **Resumos AutomÃ¡ticos**: SÃ­nteses diÃ¡rias por categoria

## ðŸ› ï¸ Tecnologias Utilizadas

| Componente | Tecnologia | VersÃ£o | Finalidade |
|------------|------------|--------|------------|
| **Linguagem** | ðŸ Python | 3.10+ | Desenvolvimento principal |
| **Web Scraping** | ðŸŽ­ Playwright | Latest | ConteÃºdo dinÃ¢mico JS |
| **Web Scraping** | ðŸ² BeautifulSoup + Requests | Latest | ConteÃºdo estÃ¡tico |
| **Dados** | ðŸ¼ Pandas | Latest | ManipulaÃ§Ã£o de dados |
| **IA/LLM** | ðŸ¤– OpenAI GPT | 4.0+ | AnÃ¡lise de sentimento e categorizaÃ§Ã£o |
| **IA/LLM** | ðŸ§  Anthropic Claude | 3.5+ | Alternativa para classificaÃ§Ã£o |
| **IA Local** | ðŸ¦™ Ollama | Latest | Modelos locais (opcional) |
| **OrquestraÃ§Ã£o** | ðŸŒ¬ï¸ Apache Airflow | 2.x | Agendamento e monitoramento |
| **Banco de Dados** | ðŸ˜ PostgreSQL | 15+ | Armazenamento (Bronze Layer) |
| **TransformaÃ§Ã£o** | ðŸ”§ DBT | 1.x | Modelagem (Silver/Gold) |
| **VisualizaÃ§Ã£o** | ðŸ“Š Streamlit | Latest | Dashboard interativo |
| **ContainerizaÃ§Ã£o** | ðŸ³ Docker | Latest | Isolamento de ambiente |
| **OrquestraÃ§Ã£o** | ðŸ™ Docker Compose | Latest | Gerenciamento de containers |

## ðŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ðŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## ðŸ“§ Contato

**Projeto**: AirDataPipeline  
**RepositÃ³rio**: https://github.com/edu-data-dev/AirDataPipeline



## ðŸ“‹ Quadro de Tarefas e Fluxo do Projeto

Para acompanhar o fluxo de criaÃ§Ã£o, execuÃ§Ã£o e todas as tarefas realizadas, acesse o quadro do Trello:

[ðŸ”— Trello - Projeto Engenharia de Dados 01](https://trello.com/b/JTQZjq00/projeto-eng-dados01)
*Este projeto demonstra competÃªncias essenciais em Engenharia de Dados: coleta automatizada, orquestraÃ§Ã£o, modelagem de dados e visualizaÃ§Ã£o, utilizando ferramentas padrÃ£o da indÃºstria.*